# Defaults for training with train.py.
# lint: disable=bad-import-order
from __gin__ import dynamic_registration
import __main__ as train_script
import seqio
from t5x import gin_utils
from t5x import partitioning
from t5x import trainer
from t5x import utils

# ------------------- REQUIRED TO BE SET ---------------------------------------
# All required macros set by separate gin files
# Must include a gin file from ./models and one from ./tasks

# From ./models/*.gin
MODEL = %gin.REQUIRED
MODEL_DIR = %gin.REQUIRED

# From ./tasks/*.gin
TRAIN_TASK_NAME = %gin.REQUIRED
TRAIN_EVAL_TASK_NAME = %gin.REQUIRED
INFER_EVAL_TASK_NAME = %gin.REQUIRED
TASK_FEATURE_LENGTHS = %gin.REQUIRED
TRAIN_STEPS = %gin.REQUIRED


# ------------------- COMMONLY OVERRIDDEN --------------------------------------
# Commonly overridden options:
# - BATCH_SIZE
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
# - USE_CACHED_TASKS: Whether to look for preprocessed SeqIO data, or preprocess
#    on the fly.

# Commonly overridden
USE_CACHED_TASKS = True
BATCH_SIZE = 256
NUM_MICROBATCHES = None
INFERENCE_BATCH_SIZE = %BATCH_SIZE  # Allow separate inference batch size.

# Sometimes overridden
EVAL_STEPS = 20

# Convenience overrides.
EVALUATOR_USE_MEMORY_CACHE = True
EVALUATOR_NUM_EXAMPLES = None  # Use all examples in the infer_eval dataset.
JSON_WRITE_N_RESULTS = 0  # Don't write any inferences.

# ------------------- Main Training Function -----------------------------------
train_script.train:
  model = %MODEL  # imported from separate gin file
  model_dir = %MODEL_DIR
  train_dataset_cfg = @train/utils.DatasetConfig()
  train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
  infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()
  checkpoint_cfg = @utils.CheckpointConfig()
  partitioner = @partitioning.PjitPartitioner()
  trainer_cls = @trainer.Trainer
  total_steps = %TRAIN_STEPS
  eval_steps = %EVAL_STEPS
  eval_period = 10000
  random_seed = None  # use faster, hardware RNG. (set to int to bypass)
  xprof_seconds = 5  # GOOGLE-INTERNAL
  summarize_config_fn = @gin_utils.summarize_gin_config
  inference_evaluator_cls = @seqio.Evaluator

  # Only needed for gaussian model.
  # random_seed = 42  # faster hardware RNG breaks TFP, use slower RNG.

# ------------------- Datasets -------------------------------------------------
train/utils.DatasetConfig:
  mixture_or_task_name = %TRAIN_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  shuffle = True
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = False

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %TRAIN_EVAL_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'eval'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = False

infer_eval/utils.DatasetConfig:
  mixture_or_task_name = %INFER_EVAL_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'eval'
  batch_size = %INFERENCE_BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = False

# ------------------- Optimization ---------------------------------------------
trainer.Trainer:
  num_microbatches = %NUM_MICROBATCHES
  learning_rate_fn = @utils.create_learning_rate_scheduler()

utils.create_learning_rate_scheduler:
  factors = 'constant'
  base_learning_rate = 0.001
  warmup_steps = 1000

partitioning.PjitPartitioner:
  num_partitions = 1
  model_parallel_submesh = None

# ------------------- Checkpoints ----------------------------------------------
utils.CheckpointConfig:
  restore = None
  save = @utils.SaveCheckpointConfig()

utils.SaveCheckpointConfig:
  period = 10000
  dtype = 'float32'
  keep = None  # keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state

# ------------------- Logging --------------------------------------------------
seqio.Evaluator:
  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]
  num_examples = %EVALUATOR_NUM_EXAMPLES
  use_memory_cache = %EVALUATOR_USE_MEMORY_CACHE

seqio.JSONLogger:
  write_n_results = %JSON_WRITE_N_RESULTS

